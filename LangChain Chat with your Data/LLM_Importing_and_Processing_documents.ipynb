{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39782a8",
   "metadata": {},
   "source": [
    "## Part 1: Document Loading\n",
    "\n",
    "Type of documents covered:\n",
    "- PDFs\n",
    "- Youtube Videos\n",
    "- Website \n",
    "\n",
    "Import and stardardise such that we obtain:\n",
    "- Content\n",
    "- Meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Connecting to account\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(\"C:/Users/richi/OneDrive/Documents/OpenAI API practice/openai_api_key.env\")) # read local .env file\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e948a",
   "metadata": {},
   "source": [
    "### PDF\n",
    "\n",
    "Each page is a Document. A Document contains text (page_content) and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b009457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pypdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e298899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"C:/Users/richi/OneDrive/Documents/OpenAI API practice/Langchain for LLM applications/Deep_Learning_A4.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521949ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pages[0]\n",
    "page.page_content[0:500] # number of characters\n",
    "page.metadata # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da022a",
   "metadata": {},
   "source": [
    "### Youtube (Broken - to be fixed)\n",
    "- Issue: yt_dlp can't find the ffmpeg files, even though they're properly installed on the local device. Didn't resolve why yet...\n",
    "- Tutorial used: https://www.youtube.com/watch?v=IECI72XEox0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install yt_dlp\n",
    "# ! pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import OpenAIWhisperParser # converts youtube audio to text format (langchain model)\n",
    "from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fed022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtube URL (video: Josh Angrist: What's the Difference Between Econometrics and Data Science? - 2 min)\n",
    "url = \"https://www.youtube.com/watch?v=2EhRT2mOXm8&t=2s\"\n",
    "\n",
    "# Directory where to save audio\n",
    "save_dir = \"C:/Users/richi/OneDrive/Documents/OpenAI API practice/Langchain for LLM applications/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a8b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOESN'T WORK FOR SOME REASON - installed fmpeg stuff...\n",
    "\n",
    "# Note: may take a while & will give error if the content is already present/downloaded in file directory\n",
    "loader = GenericLoader(\n",
    "    YoutubeAudioLoader([url], save_dir),\n",
    "    OpenAIWhisperParser()\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eda6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = docs[0]\n",
    "doc.page_content[0: 500]\n",
    "doc.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c685914",
   "metadata": {},
   "source": [
    "### URLs\n",
    "\n",
    "Note: format is probably really poorly formatted, so we should post-process for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cbf28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://richie-lee.github.io/post/2021_uplift/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45942e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa8477",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = docs[0]\n",
    "doc.page_content[:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343d3e8",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Document splitting\n",
    "\n",
    "*After* loading data and *before* feeding it into the vector store \n",
    "\n",
    "Fundamental concept: splitting on chunks with some size, with overlap. This overlap is helpful in ensuring no information is loss when splitting texts.\n",
    "\n",
    "Types of splitting:\n",
    "- **CharacterTextSplitter():** based on characters\n",
    "- **MarkdownHeaderTextSplitter():** based on MD headers\n",
    "- **TokenTextSplitter():** based on tokens\n",
    "- **RecursiveCharacterTextSplitter():** recursively tries to split by different characters to see what works\n",
    "- **Language():** for Python, Ruby, Markdown, ...\n",
    "- **NLTKTextSplitter():** based on sentences and NLTK (natural language tool kit)\n",
    "- **SpacyTextSplitter():** based on sentences and Spacy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Connecting to account\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(\"C:/Users/richi/OneDrive/Documents/OpenAI API practice/openai_api_key.env\")) # read local .env file\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607689c2",
   "metadata": {},
   "source": [
    "Intuitive examples of (Recursive) character text splitters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05354f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size =26\n",
    "chunk_overlap = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise two different text splitters\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf90893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n < 26 (chunk size)\n",
    "text1 = 'abcdefghijklmnopqrstuvwxyz'\n",
    "print(r_splitter.split_text(text1))\n",
    "\n",
    "# n > 26 (chunk size)\n",
    "text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'\n",
    "print(r_splitter.split_text(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eece7b2",
   "metadata": {},
   "source": [
    "character text splitting issue: it splits on a new characters, by default a newline char, but here there arent't any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a327fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
    "print(r_splitter.split_text(text3)) # recursive character text splitting\n",
    "print(c_splitter.split_text(text3)) # character text splitting: issue it splits on a new characters, by default a newline char, but here there arent't any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - given processing C & R become equivalent\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separator = ' '\n",
    ")\n",
    "c_splitter.split_text(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490afd25",
   "metadata": {},
   "source": [
    "Recursive splitting details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d14a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ef471",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d876bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0,\n",
    "    separator = ' '\n",
    ")\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=0, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # default sepearators, but here for illustration explicitly displayed - it moves from left to right recursively\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only splits on spaces\n",
    "c_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4156d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits on \\n\\n first, and then rest respectively for better quality due to importance hierarchy\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fcdfee",
   "metadata": {},
   "source": [
    "For periods, define regex with lookback for better results: \"(?<=\\.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4c5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"] # separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"] not this due to REGEX under the hood\n",
    ")\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524560b0",
   "metadata": {},
   "source": [
    "Try with real example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a07b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"C:/Users/richi/OneDrive/Documents/OpenAI API practice/Langchain for LLM applications/Deep_Learning_A4.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e22a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To illustrate difference it may make\n",
    "print(len(docs), len(pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ea27c",
   "metadata": {},
   "source": [
    "**Token splitting:** LLMs often have context windows designated in tokens (approx 4 characters often)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431870f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64648f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"foo bar bazzyfoo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2495a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d64d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8285436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note metadata is same in chunk as in pages (which is good).\n",
    "docs[0]\n",
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b4ce4",
   "metadata": {},
   "source": [
    "**Context aware splitting:** adds meta data to the text chunks\n",
    "\n",
    "- chunks aim to keep text with common context together\n",
    "- text splitting often uses sentences or other delimiters to keep related text together, but some docs have explicit structures that can be used (e.g. markdown headers) - headers become metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec8d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a3781",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_document = \"\"\"# Title\\n\\n \\\n",
    "## Chapter 1\\n\\n \\\n",
    "Hi this is Jim\\n\\n Hi this is Joe\\n\\n \\\n",
    "### Section \\n\\n \\\n",
    "Hi this is Lance \\n\\n \n",
    "## Chapter 2\\n\\n \\\n",
    "Hi this is Molly\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb86422",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7674a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d9eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(md_header_splits)) # number chunks\n",
    "\n",
    "print(md_header_splits[0]) # first chunk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
