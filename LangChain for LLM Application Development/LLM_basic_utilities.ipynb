{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74dcdaf9",
   "metadata": {},
   "source": [
    "# PART 1: Basics of models, prompts & parsers\n",
    "\n",
    "- **Openai_api_key:** set up using .env file\n",
    "- **Templating prompts & outputs**\n",
    "- **Parsing**: Extracting specific info from outputs\n",
    "- **Formatting**: writing prompts to improve inferential and generative ability of LLM (ReAct: Thought, Action, Observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791aaef",
   "metadata": {},
   "source": [
    "Import packages & specify basic completion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42ee07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Connecting to account\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(\"C:/Users/richi/OneDrive/Documents/OpenAI API practice/openai_api_key.env\")) # read local .env file\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67caf21c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uses GPT 3.5 Turbo\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}] \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c23f2ea",
   "metadata": {},
   "source": [
    "Basic example use-case: translate and set tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d59cd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "message = \"\"\"\n",
    "Hey iedereen, met Karyan van Takumi. Morgen is het zover\\\n",
    ", we hebben er veel zin in! üì∏Carmen, Rachel, Yip en Richie we zien elkaar 10:30 bij Takumi Nieuwstraat.\\\n",
    "Mochten er nog vragen zijn vandaag, stel ze gerustüôãüèª‚Äç‚ôÄ¬†Tot¬†morgen\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c04bb4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "style = \"\"\" \n",
    "American English \\\n",
    "in a concise and confident tone, like a strict business executive\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6309f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{message}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e6b96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c9e67a",
   "metadata": {},
   "source": [
    "## Langchain\n",
    "Building a sequence of comptatible requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03e5ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install --upgrade Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9f95a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb554d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To control the randomness: temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "print(chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf54327",
   "metadata": {},
   "source": [
    "## Prompt template\n",
    "Useful for reusing long/useful prompts (with a layer of abstraction) with placeholder we can change along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d3c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbac45c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93ee29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c965c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Hey bro, what the fuck is this a normal price, \n",
    "but you're employee charged me double the original price\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d6092e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a190b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_response = chat(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277c1fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1cd0e8",
   "metadata": {},
   "source": [
    "## Formatting\n",
    "Chain-of-thought reasoning (ReAct):\n",
    "- Thought: helps form better answers\n",
    "- Action\n",
    "- Observation: show what it learned from action \n",
    "\n",
    "Couple parser to extract interpret the output of these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac54e0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example output format (Dictionary / JSON)\n",
    "\n",
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f1383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "# Template to eventually get to JSON\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5714001",
   "metadata": {},
   "source": [
    "How to wrap this in Langchain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54abf7b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4018c885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "chat = ChatOpenAI(temperature=0.0) # add api key as parameter (\"openai_api_key\") if not as .env parameter (recommended instead)\n",
    "response = chat(messages) # Get response (note: output is type STRING)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e267fd56",
   "metadata": {},
   "source": [
    "Output type here was STRING, now we want to parse to a dict/json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef27c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcec476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797bd39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe6541",
   "metadata": {},
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0c93b",
   "metadata": {},
   "source": [
    "Now we can get the output compressed in a dictionary object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a242ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Raw output\n",
    "response = chat(messages)\n",
    "# Output parser\n",
    "output_dict = output_parser.parse(response.content)\n",
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e929fc8",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: Memory\n",
    "\n",
    "- **ConversationBufferMemory:** store/extract messages as variables\n",
    "- **ConversationBufferWindowMemory:** keep last *k* interactions as context/memory\n",
    "- **ConversationTokenBufferMemory:** keep last interactions, capped by specified number of tokens as context/memory\n",
    "- **ConversationSummaryMemory:** create summary of conversation as context/memory\n",
    "\n",
    "additionally, Langchain also supports other memory types (beyond the scope of this notebook):\n",
    "- **Vector Data Memory:** Stores text in vector database and retrieves most relevant blocks of text\n",
    "- **Entity Memory:** Using LLM, remembers details about specific entities (objects)\n",
    "\n",
    "Typically, if stored in databases, conversations are stored. They are stored as key-value pairs in SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b08691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Connecting to account\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(\"C:/Users/richi/OneDrive/Documents/OpenAI API practice/openai_api_key.env\")) # read local .env file\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ad156",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory (1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d73a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61e1f3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Builds LLM\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose= True # With this you get insight in which prompts are ran under the hood (if True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3f8871",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conversation.predict(input = \"Hi, I'm Richie\")\n",
    "\n",
    "conversation.predict(input = \"do you remember my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f5f030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436724d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({}) # {} = empty dictionary, handy for more advanced use-cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a29831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b1f602",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf0d212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef94d69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c810a",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory (2/4)\n",
    "LLM are stateless: i.e. each transaction is independent\n",
    "Memory storage is used as additional **context** to have more conversation-like content.\n",
    "\n",
    "A lot of context, directly implies **more tokens** and costs are charged by number of tokens consumed. Hence we should manage it well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67846408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddea251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k = 1) # k = iteration it keeps (1 input + 1 output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deb78a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c925ad97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a3d8b",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory (3/4)\n",
    "Memory limits nr of tokens saved to manage costs\n",
    "\n",
    "- different LLM use different ways of counting tokens, hence we initialise a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacaf26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e1b2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0) # different LLM use different ways of counting tokens, hence we initialise a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff41f09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample conversation\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100) # specify token limit here (priortises recent exchanges)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff6c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b41ea",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory (4/4)\n",
    "\n",
    "Using LLM to write a summary of past conversation and using that as memory/context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f66439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b26d128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "# Insert few conversational terms\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6195ed8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summary of the conversation so far\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94911bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verbose doesn't show actual OpenAI message, but still good.\n",
    "Conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a724a8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now make a request, including the memory\n",
    "Conversation.predict(input = \"what would be a nice demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b35703f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note: latest message is now included\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a9dbc",
   "metadata": {},
   "source": [
    "Keep specific storage of messages up the limit of tokens. and uses CLM (casual language model?) to generate and save past data as summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82677a8a",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 3: Chains\n",
    "\n",
    "Different ways to compounding multiple prompts \n",
    "- **Simple Chain**\n",
    "- **Sequential Chain:** multiple inputs / multiple outputs\n",
    "- **Router Chain:** allocate traffic along various different chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58645aa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Connecting to account\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(\"C:/Users/richi/OneDrive/Documents/OpenAI API practice/openai_api_key.env\")) # read local .env file\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fc2116",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df2b29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6bf0d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa65d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e448ee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da9fce0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "product = \"Blenders with funny designs\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad3cd4",
   "metadata": {},
   "source": [
    "## Simple Sequential Chains\n",
    "combine chains where output chain 1, is input chain 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76786c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b5b49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684fba2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129cdf8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
    "                                             verbose=True\n",
    "                                            )\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee450f3",
   "metadata": {},
   "source": [
    "## SequentialChain\n",
    "\n",
    "Multiple inputs and multiple outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c363b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9182135f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"English_Review\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75320957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc797fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f25c50e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c089e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57dffd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review = \"Located in the Rotterdam Food Hall this wonderful Vietnamese stall serves excellent and authentic Vietnamese fare. Excellent service, reasonably priced and scrumptious food. Highly recommended\"\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa05833",
   "metadata": {},
   "source": [
    "## Router Chain\n",
    "Allocate inputs to sub-route chains, dependent on situations.\n",
    "\n",
    "If no chain seems appropriate: **default chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd90a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b977f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Templates\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c0fe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa44cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e925f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f87f2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb260d4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(), # Important to help chain decide how to direct traffic amongst sub-chains\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa6425",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aefd0b9",
   "metadata": {},
   "source": [
    "warning:\n",
    "*C:\\Users\\richi\\anaconda3\\Lib\\site-packages\\langchain\\chains\\llm.py:279: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf2681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try questions\n",
    "chain.run(\"What's your favorite character in naruto?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc1392b",
   "metadata": {},
   "source": [
    "---\n",
    "## PART 4: Questions & Answers over documents\n",
    "Working with embeddings & vector stores to handle big datasets / inputs\n",
    "\n",
    "**Embedding methods:**\n",
    "- **Stuff** (simple, not good for big data)\n",
    "- **Map-reduce** (Summarise independent responses in single response, costs more calls, treats all chunks/documents as independent)\n",
    "- **Refine** (Iteratively building up answers, will lead to long answers, slow due to dependency, same number of call as map-reduce)\n",
    "- **Map-rerank** (Do all individual calls, rank and select highest as in softmax)\n",
    "\n",
    "**Terminology**\n",
    "- **Retriever**: interface to interact with documents (note: often the cause for poor outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57c083",
   "metadata": {},
   "source": [
    "![](https://github.com/Richie-Lee/LLM_practice/blob/main/LangChain%20for%20LLM%20Application%20Development/embedding_llm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e47a82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Connecting to account\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(\"C:/Users/richi/OneDrive/Documents/OpenAI API practice/openai_api_key.env\")) # read local .env file\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb05b084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown # common display utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a959c398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath = \"C:/Users/richi/Downloads/Cereals.csv\"\n",
    "cereal_loader = CSVLoader(file_path = filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1783eb55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator # helps create vector stoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8566b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f364c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify vector store class\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls = DocArrayInMemorySearch, # easy one\n",
    ").from_loaders([cereal_loader]) # specify all the docs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b7586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To check whether the csv is not broken\n",
    "import pandas as pd\n",
    "file_path = 'C:/Users/richi/Downloads/Cereals.csv'\n",
    "df = pd.read_csv(file_path) # Read the CSV file into a DataFrame\n",
    "print(df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347d177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query over document (here csv) in loader\n",
    "query =\"Please list the average sodium value per mfr\\\n",
    "in a table in markdown\"\n",
    "response = index.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be062a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using markdown visualisation package\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca7086c",
   "metadata": {},
   "source": [
    "**Embedding practice**\n",
    "\n",
    "- Divide in chunks (Here: Docs / CSV rows) and allocate content to best content-matching chunk\n",
    "- retrievers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ed5fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b9562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = cereal_loader.load() # each page = 1 CSV row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f1155",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example that shows what the embedding may look like\n",
    "embed = embeddings.embed_query(\"Hi, I'm richie\")\n",
    "print(f\"length: {len(embed)}, snippet: {embed[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8c7dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create embeddings for all pieces of text, and save in vector store & \"from-document method\"\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, \n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d5c7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Based on the query it redirects to the embeddings with best matches (docs)\n",
    "query = \"Type C fruit\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abaee4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using this to Q&A - retriever is generic interface to fetch documents\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe9406e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for NLP responses\n",
    "llm = ChatOpenAI(temperature = 0) # Temperature controls randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a46ffa",
   "metadata": {},
   "source": [
    "Helpful ideas to scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f386cc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine documents in single piece of text:\n",
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12eb24a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = llm.call_as_llm(f\"{qdocs} Question: Please list all your \\\n",
    "fruit product in a table in markdown and summarize each one.\") \n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da89f1c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# can be encapsulated in langchain chain,\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", # different processing methods (stuff = simplest)\n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00400b76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run chain on query\n",
    "query =  \"Please list all the products with a sodium level above 100 \\\n",
    "in markdown and summarize each one.\"\n",
    "\n",
    "response = qa_stuff.run(query)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bfbdbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ab55d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Above was manual embedding creation, we can also do it more compactly as follows:\n",
    "response = index.query(query, llm=llm)\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings,\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6cb8b1",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 5: Evaluation \n",
    "*Prompting-based* development. Using LLM to extract semantic meanings (\"new\" evaluation heuristic)\n",
    "\n",
    "- Wrong answer, often cause of poor retrieval, rather than inadequate model\n",
    "- debugging for context interpretability & token usage (cost management)\n",
    "- Manually/LLM-assisted example creation as labelled data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75383c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Connecting to account\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(\"C:/Users/richi/OneDrive/Documents/OpenAI API practice/openai_api_key.env\")) # read local .env file\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3c46c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f091c54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath = \"C:/Users/richi/Downloads/Cereals.csv\"\n",
    "cereal_loader = CSVLoader(file_path = filepath)\n",
    "data = cereal_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b259b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5241dcab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=index.vectorstore.as_retriever(), \n",
    "    verbose=True,\n",
    "    chain_type_kwargs = {\n",
    "        \"document_separator\": \"<<<<>>>>>\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec912c",
   "metadata": {},
   "source": [
    "**Example based evaluation: Manual**\n",
    "\n",
    "Come up with good examples (Q&A pairs): \n",
    "- example input\n",
    "- example ground truth (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e43ffde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Manual\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Are there products that contain apples?\",\n",
    "        \"answer\": \"Yes, we have Apple Cinnamon Cheerios and Apple Jacks\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the top 3 highest rated products?\",\n",
    "        \"answer\": \"Here is a list with the top 5: 1. All-Bran with Extra Fiber, 2. Shredded Wheat nBran, 3. Shredded Wheat spoon size.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a5e341",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automate: \n",
    "from langchain.evaluation.qa import QAGenerateChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a06e0a",
   "metadata": {},
   "source": [
    "**Example based evaluation: Automated/Assisted using LLMs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c956de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Takes in document and creates question/answer pairs using a LLM\n",
    "example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI())\n",
    "\n",
    "new_examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": t} for t in data[:5]]\n",
    ")\n",
    "\n",
    "# UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b531b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_examples[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de9eb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add to existing examples\n",
    "examples += new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde457f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Shows inside the chain (limiting info though)\n",
    "qa.run(examples[0][\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0751cf35",
   "metadata": {},
   "source": [
    "**Debugging**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2ba76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helps interpret what's happening under the hood - showing what's happening under the hood\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "qa.run(examples[0][\"query\"])\n",
    "\n",
    "# Often when something goes wrong, it's not the model what's wrong but thr retrieval steps\n",
    "# Tracks tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3160e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Turn off the debug mode\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3729695",
   "metadata": {},
   "source": [
    "LLM assisted evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88093af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predictions for all examples \n",
    "predictions = qa.apply(examples)\n",
    "\n",
    "# Fails -> appears to struggle with generated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af15045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create evaluation chain with language model\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "eval_chain = QAEvalChain.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad32fd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graded_outputs = eval_chain.evaluate(examples, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9955fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, eg in enumerate(examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08427b1a",
   "metadata": {},
   "source": [
    "**Example of how it should look:**\n",
    "\n",
    "Example 0:\n",
    "Question: Do the Cozy Comfort Pullover Set have side pockets?\n",
    "Real Answer: Yes\n",
    "Predicted Answer: The Cozy Comfort Pullover Set, Stripe does have side pockets.\n",
    "Predicted Grade: CORRECT\n",
    "\n",
    "Example 1:\n",
    "Question: What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?\n",
    "Real Answer: The DownTek collection\n",
    "Predicted Answer: The Ultra-Lofty 850 Stretch Down Hooded Jacket is from the DownTek collection.\n",
    "Predicted Grade: CORRECT\n",
    "\n",
    "Example 2:\n",
    "Question: What is the weight of each pair of Women's Campside Oxfords?\n",
    "Real Answer: The approximate weight of each pair of Women's Campside Oxfords is 1 lb. 1 oz.\n",
    "Predicted Answer: The weight of each pair of Women's Campside Oxfords is approximately 1 lb. 1 oz.\n",
    "Predicted Grade: CORRECT\n",
    "\n",
    "Example 3:\n",
    "Question: What are the dimensions of the small and medium Recycled Waterhog Dog Mat?\n",
    "Real Answer: The dimensions of the small Recycled Waterhog Dog Mat are 18\" x 28\" and the dimensions of the medium Recycled Waterhog Dog Mat are 22.5\" x 34.5\".\n",
    "Predicted Answer: The small Recycled Waterhog Dog Mat has dimensions of 18\" x 28\" and the medium size has dimensions of 22.5\" x 34.5\".\n",
    "Predicted Grade: CORRECT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abceb970",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 6: Agents (reasoning engines)\n",
    "\n",
    "Build (debuggable) tools, that leverage chains that resemble logic. \n",
    "\n",
    "Examples covered:\n",
    "- Calculator\n",
    "- Wikipedia scraping (connecting wikipedia API)\n",
    "\n",
    "Also covers how to customise tools/agents of your own\n",
    "- Here: get today's date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411d077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Connecting to account\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv(\"C:/Users/richi/OneDrive/Documents/OpenAI API practice/openai_api_key.env\")) # read local .env file\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e32a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0) # Temperature = 0, to remove randomness in (carefully designed) reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4283c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# API running against wikipedia\n",
    "# pip install -U wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f2d35",
   "metadata": {},
   "source": [
    "**Calculator & Wikipedia search:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tools (LLM-math = calculator, Wikipedia = API to search wikipedia)\n",
    "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent= initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, # CHAT: optimised for chat_models, REACT: designed to get best reasoning\n",
    "    handle_parsing_errors=True, # Useful when output is not parsable to action -> pass misformatted text back to LLM to correct itself\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math test (good to understand what's happening the hood)\n",
    "agent(\"What is the 25% of 300?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eaf4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Wikipedia API test\n",
    "question = \"I'm interested in learning about naruto, who is the main female character in this series?\"\n",
    "result = agent(question) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96160e7",
   "metadata": {},
   "source": [
    "**Python Agent:**\n",
    "\n",
    "REPL = way to interact with code (roughly speaking like Jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_python_agent(\n",
    "    llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba836e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_list = [[\"Harrison\", \"Chase\"], \n",
    "                 [\"Lang\", \"Chain\"],\n",
    "                 [\"Dolly\", \"Too\"],\n",
    "                 [\"Elle\", \"Elem\"], \n",
    "                 [\"Geoff\",\"Fusion\"], \n",
    "                 [\"Trance\",\"Former\"],\n",
    "                 [\"Jen\",\"Ayai\"]\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e62d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View detailed outputs of the chains\n",
    "import langchain\n",
    "langchain.debug=True\n",
    "agent.run(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\") \n",
    "langchain.debug=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d58a5e",
   "metadata": {},
   "source": [
    "**Defining a personalised Tool**\n",
    "\n",
    "Connect to own sources / API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ebe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b03137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfefc114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a custom tool -> make sure to design the prompt such that the LLM will know when to direct traffic to this tool \n",
    "@tool\n",
    "def time(text: str) -> str:\n",
    "    \"\"\"Returns todays date, use this for any \\\n",
    "    questions related to knowing todays date. \\\n",
    "    The input should always be an empty string, \\ \n",
    "    and this function will always return todays \\\n",
    "    date - any date mathmatics should occur \\\n",
    "    outside this function.\"\"\"\n",
    "    return str(date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33316d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: when not reaching an answer - token usage can inflate if not bounded (max 4097 tokens)\n",
    "agent= initialize_agent(\n",
    "    tools + [time], \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)\n",
    "\n",
    "agent.run(\"What's the date today?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19bbdd2",
   "metadata": {},
   "source": [
    "Note: Agents may reach wrong conclusions (currently still WIP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun if wrong answer is reached\n",
    "try:\n",
    "    result = agent(\"whats the date today?\") \n",
    "except: \n",
    "    print(\"exception on external access\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
